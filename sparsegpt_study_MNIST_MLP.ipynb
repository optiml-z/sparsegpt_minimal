{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\igor-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_runtime.py:185: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from modelutils import *\n",
    "from quant import *\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"dynamic_mnist_mlp.pth\"\n",
    "LOAD = True\n",
    "\n",
    "if not(LOAD):\n",
    "    SAVE = True\n",
    "else:\n",
    "    SAVE = False\n",
    "\n",
    "DEBUG = True \n",
    "\n",
    "@dataclass\n",
    "class Args(object):\n",
    "    nsamples: int = 8\n",
    "    sparsity = 0.3\n",
    "    prunen: int = 0\n",
    "    prunem: int = 0\n",
    "    percdamp = .01\n",
    "    blocksize: int = 4\n",
    "    batch_size: int = 8\n",
    "    num_layers: int = 5\n",
    "    input_size: int = 784\n",
    "    output_size: int = 10\n",
    "    \n",
    "args = Args()\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preparation\n",
    "# Define transformations and load datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "# Confirm if data is loaded\n",
    "len(trainset), len(testset)\n",
    "\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2     7     7     1     1     3     0     3    \n"
     ]
    }
   ],
   "source": [
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'\\n {classes[labels[j]]:5s}' if j%8 == 0 else ''.join(f'{classes[labels[j]]:5s}') for j in range(args.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicMLP(nn.Module):\n",
    "    def __init__(self, input_size=28*28, output_size=10, hidden_sizes=0, num_layers=5):\n",
    "        super(DynamicMLP, self).__init__()\n",
    "        \n",
    "        # Ensure hidden_sizes is a list for consistency\n",
    "        if not isinstance(hidden_sizes, list):\n",
    "            hidden_sizes = [input_size] * num_layers\n",
    "        elif len(hidden_sizes) == 0:\n",
    "            hidden_sizes += [input_size] * (num_layers - len(hidden_sizes))\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(1, num_layers):\n",
    "            layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[min(i, len(hidden_sizes)-1)]))\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "        # ModuleList to hold all layers\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply each layer with ReLU activation, except for the output layer\n",
    "        x = x.view(-1, 28*28) # Flatten the image        \n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        # Output layer without ReLU activation\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "    \n",
    "class MLPSeq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPSeq, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(28**2, 28**2) for i in range(5)])        \n",
    "        # self.linears.insert(0, nn.Flatten())\n",
    "        # self.fc_block1 = nn.Sequential(\n",
    "        #     nn.Linear(28*28, 28*28),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(28*28, 28*28),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "        # self.fc_block2 = nn.Sequential(\n",
    "        #     nn.Linear(28*28, 28*28),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(28*28, 28*28),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "        self.out = nn.Linear(28*28, 10)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28) # Flatten the image\n",
    "        for layer in self.linears:\n",
    "            x = torch.relu(layer(x))\n",
    "        # x = self.fc_block1(x)\n",
    "        # x = self.fc_block2(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DynamicMLP(num_layers=args.num_layers).to(device)\n",
    "# model = MLPSeq().to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 1\n",
    "train_losses, test_losses = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD:\n",
    "    PATH = f'./{MODEL_NAME}'\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "else:    \n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images.to(device))\n",
    "            loss = criterion(output, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in testloader:\n",
    "                    log_ps = model(images.to(device))\n",
    "                    test_loss += criterion(log_ps, labels.to(device))\n",
    "                    ps = torch.exp(log_ps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.to(device).view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                    \n",
    "            train_losses.append(running_loss/len(trainloader))\n",
    "            test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "            print(f\"Epoch {e+1}/{epochs}.. \"\n",
    "                f\"Train loss: {running_loss/len(trainloader):.3f}.. \"\n",
    "                f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "                f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
    "\n",
    "    print('Finished Training')\n",
    "    if SAVE:\n",
    "        PATH = f'./{MODEL_NAME}'\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "        plt.plot(train_losses, label='Training loss')\n",
    "        plt.plot(list(map(torch.Tensor.cpu, test_losses)), label='Validation loss')\n",
    "        plt.legend(frameon=False)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples tested = 1000\n",
      "\n",
      "Model Accuracy = 0.914\n"
     ]
    }
   ],
   "source": [
    "def infer_and_compute_accuracy_random_samples(model, dataset, num_samples=1000):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Create a list of indices and shuffle them\n",
    "    indices = list(range(len(dataset)))\n",
    "    np.random.shuffle(indices)\n",
    "    idx = indices[:num_samples]\n",
    "\n",
    "    # Create a DataLoader with SubsetRandomSampler\n",
    "    sampler = SubsetRandomSampler(idx)\n",
    "    testloader_random_samples = DataLoader(dataset, batch_size=64, sampler=sampler)\n",
    "\n",
    "    correct_count, all_count = 0, 0\n",
    "    for images, labels in testloader_random_samples:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images.view(images.shape[0], -1).to(device))\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_count += (predicted == labels.to(device)).sum().item()\n",
    "        all_count += labels.size(0)\n",
    "    \n",
    "    print(\"Number of Samples tested =\", all_count)\n",
    "    print(\"\\nModel Accuracy =\", (correct_count / all_count))\n",
    "\n",
    "# Assuming testset is your test dataset\n",
    "# Call the function\n",
    "infer_and_compute_accuracy_random_samples(model, testset, num_samples=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](torch.randn(1, 28*28).to(device)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseGPT:\n",
    "\n",
    "    def __init__(self, layer):\n",
    "        self.layer = layer\n",
    "        print(layer)\n",
    "        print(type(layer))\n",
    "        self.dev = self.layer.weight.device\n",
    "        W = layer.weight.data.clone()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            W = W.flatten(1)\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        self.rows = W.shape[0]\n",
    "        self.columns = W.shape[1]\n",
    "        self.H = torch.zeros((self.columns, self.columns), device=self.dev)\n",
    "        self.nsamples = 0\n",
    "\n",
    "    def add_batch(self, inp, out, blocksize=1024):\n",
    "        if DEBUG:\n",
    "            self.inp1 = inp\n",
    "            self.out1 = out\n",
    "        if len(inp.shape) == 2:\n",
    "            inp = inp.unsqueeze(0)\n",
    "        tmp = inp.shape[0]\n",
    "        if isinstance(self.layer, nn.Linear) or isinstance(self.layer, transformers.Conv1D):\n",
    "            if len(inp.shape) == 3:\n",
    "                inp = inp.reshape((-1, inp.shape[-1]))\n",
    "            inp = inp.t()\n",
    "        self.H *= self.nsamples / (self.nsamples + tmp)\n",
    "        self.nsamples += tmp\n",
    "        inp = math.sqrt(2 / self.nsamples) * inp.float()\n",
    "        self.H += inp.matmul(inp.t())\n",
    "\n",
    "    def fasterprune(\n",
    "        self, sparsity, prunen=0, prunem=0, blocksize=128, percdamp=.01\n",
    "    ):\n",
    "        W = self.layer.weight.data.clone()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            W = W.flatten(1)\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        W = W.float()\n",
    "\n",
    "        if hasattr(self, 'quantizer'):\n",
    "            if not self.quantizer.ready():\n",
    "                self.quantizer.find_params(W, weight=True)\n",
    "\n",
    "        tick = time.time()\n",
    "\n",
    "        H = self.H\n",
    "        del self.H\n",
    "        dead = torch.diag(H) == 0\n",
    "        H[dead, dead] = 1\n",
    "        W[:, dead] = 0\n",
    "\n",
    "        Losses = torch.zeros(self.rows, device=self.dev)\n",
    "\n",
    "        damp = percdamp * torch.mean(torch.diag(H))\n",
    "        diag = torch.arange(self.columns, device=self.dev)\n",
    "        H[diag, diag] += damp\n",
    "        H = torch.linalg.cholesky(H)\n",
    "        H = torch.cholesky_inverse(H)\n",
    "        H = torch.linalg.cholesky(H, upper=True)\n",
    "        Hinv = H\n",
    "\n",
    "        mask = None\n",
    "\n",
    "        for i1 in range(0, self.columns, blocksize):\n",
    "            i2 = min(i1 + blocksize, self.columns)\n",
    "            count = i2 - i1\n",
    "\n",
    "            W1 = W[:, i1:i2].clone()\n",
    "            Q1 = torch.zeros_like(W1)\n",
    "            Err1 = torch.zeros_like(W1)\n",
    "            Losses1 = torch.zeros_like(W1)\n",
    "            Hinv1 = Hinv[i1:i2, i1:i2]\n",
    "\n",
    "            if prunen == 0: \n",
    "                if mask is not None:\n",
    "                    mask1 = mask[:, i1:i2]\n",
    "                else:\n",
    "                    tmp = W1 ** 2 / (torch.diag(Hinv1).reshape((1, -1))) ** 2\n",
    "                    thresh = torch.sort(tmp.flatten())[0][int(tmp.numel() * sparsity)]\n",
    "                    mask1 = tmp <= thresh\n",
    "            else:\n",
    "                mask1 = torch.zeros_like(W1) == 1\n",
    "\n",
    "            for i in range(count):\n",
    "                w = W1[:, i]\n",
    "                d = Hinv1[i, i]\n",
    "\n",
    "                if prunen != 0 and i % prunem == 0:\n",
    "                    tmp = W1[:, i:(i + prunem)] ** 2 / (torch.diag(Hinv1)[i:(i + prunem)].reshape((1, -1))) ** 2\n",
    "                    mask1.scatter_(1, i + torch.topk(tmp, prunen, dim=1, largest=False)[1], True)\n",
    "\n",
    "                q = w.clone()\n",
    "                q[mask1[:, i]] = 0\n",
    "\n",
    "                if hasattr(self, 'quantizer'):\n",
    "                    q = quantize(\n",
    "                        q.unsqueeze(1), self.quantizer.scale, self.quantizer.zero, self.quantizer.maxq\n",
    "                    ).flatten()\n",
    "\n",
    "                Q1[:, i] = q\n",
    "                Losses1[:, i] = (w - q) ** 2 / d ** 2\n",
    "\n",
    "                err1 = (w - q) / d\n",
    "                W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))\n",
    "                Err1[:, i] = err1\n",
    "\n",
    "            W[:, i1:i2] = Q1\n",
    "            Losses += torch.sum(Losses1, 1) / 2\n",
    "\n",
    "            W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])\n",
    "\n",
    "            if DEBUG:\n",
    "                self.layer.weight.data[:, :i2] = W[:, :i2]\n",
    "                self.layer.weight.data[:, i2:] = W[:, i2:]\n",
    "                print(torch.sum((self.layer(self.inp1) - self.out1) ** 2))\n",
    "                print(torch.sum(Losses))\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        print('time %.2f' % (time.time() - tick))\n",
    "        print('error', torch.sum(Losses).item())\n",
    "\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        self.layer.weight.data = W.reshape(self.layer.weight.shape).to(self.layer.weight.data.dtype)\n",
    "        if DEBUG:\n",
    "            print(torch.sum((self.layer(self.inp1) - self.out1) ** 2))\n",
    "\n",
    "    def free(self):\n",
    "        if DEBUG:\n",
    "            self.inp1 = None\n",
    "            self.out1 = None\n",
    "        self.H = None\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DynamicMLP(num_layers=args.num_layers).to(device)\n",
    "@torch.no_grad()\n",
    "def mlp_sequential(model, dataloader, dev):\n",
    "    print('Starting ...')\n",
    "\n",
    "    # use_cache = model.config.use_cache\n",
    "    # model.config.use_cache = False\n",
    "    # layers = model.transformer.h\n",
    "    layers = model.layers\n",
    "    print(\"layers: \", layers)\n",
    "    # model.transformer.word_embeddings = model.transformer.word_embeddings.to(dev)\n",
    "    # model.transformer.word_embeddings_layernorm = model.transformer.word_embeddings_layernorm.to(dev)\n",
    "    layers[0] = layers[0].to(dev)\n",
    "    \n",
    "    print(\"layers[0]: \", layers[0])\n",
    "    layers[0] = layers[0].to(dev)\n",
    "\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "    inps = torch.zeros(\n",
    "        (args.nsamples, args.batch_size, 28*28), dtype=dtype, device=dev\n",
    "    )\n",
    "    # cache = {'i': 0, 'attention_mask': None, 'alibi': None}\n",
    "    cache = {'i': 0}\n",
    "\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "        def forward(self, inp, **kwargs):\n",
    "            # print(\"inps cache: \", inps[cache['i']])\n",
    "            inps[cache['i']] = inp\n",
    "            cache['i'] += 1\n",
    "            # cache['attention_mask'] = kwargs['attention_mask']\n",
    "            # cache['alibi'] = kwargs['alibi']\n",
    "            raise ValueError\n",
    "    layers[0] = Catcher(layers[0])\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        try:\n",
    "            print(i)\n",
    "            print(batch[0].shape)\n",
    "            model(batch[0].to(dev))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    layers[0] = layers[0].module\n",
    "\n",
    "    layers[0] = layers[0].cpu()\n",
    "    # model.transformer.word_embeddings = model.transformer.word_embeddings.cpu()\n",
    "    # model.transformer.word_embeddings_layernorm = model.transformer.word_embeddings_layernorm.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    outs = torch.zeros_like(inps)\n",
    "    # attention_mask = cache['attention_mask']\n",
    "    # alibi = cache['alibi']\n",
    "\n",
    "    print('Ready.')\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i].to(dev)\n",
    "\n",
    "        subset = find_layers(layer)\n",
    "        gpts = {}\n",
    "        for name in subset:\n",
    "            if (not (args.minlayer <= i < args.maxlayer and args.prune_only in name)) == (not args.invert):\n",
    "                continue\n",
    "            gpts[name] = SparseGPT(subset[name])\n",
    "\n",
    "        def add_batch(name):\n",
    "            def tmp(_, inp, out):\n",
    "                gpts[name].add_batch(inp[0].data, out.data)\n",
    "            return tmp\n",
    "        handles = []\n",
    "        for name in gpts:\n",
    "            handles.append(subset[name].register_forward_hook(add_batch(name)))\n",
    "        for j in range(args.nsamples):\n",
    "            # outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, alibi=alibi)[0]\n",
    "            outs[j] = layer(inps[j].unsqueeze(0))[0]\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "        for name in gpts:\n",
    "            print(i, name)\n",
    "            print('pruning ...')\n",
    "            gpts[name].fasterprune(\n",
    "                args.sparsity, prunen=args.prunen, prunem=args.prunem, percdamp=args.percdamp\n",
    "            )\n",
    "        for j in range(args.nsamples):\n",
    "            # outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, alibi=alibi)[0]\n",
    "            outs[j] = layer(inps[j].unsqueeze(0))[0]\n",
    "\n",
    "        layers[i] = layer.cpu()\n",
    "        del gpts \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        inps, outs = outs, inps\n",
    "\n",
    "    # model.config.use_cache = use_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ...\n",
      "layers:  ModuleList(\n",
      "  (0-4): 5 x Linear(in_features=784, out_features=784, bias=True)\n",
      "  (5): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "layers[0]:  Linear(in_features=784, out_features=784, bias=True)\n",
      "0\n",
      "torch.Size([8, 1, 28, 28])\n",
      "1\n",
      "torch.Size([8, 1, 28, 28])\n",
      "2\n",
      "torch.Size([8, 1, 28, 28])\n",
      "3\n",
      "torch.Size([8, 1, 28, 28])\n",
      "4\n",
      "torch.Size([8, 1, 28, 28])\n",
      "5\n",
      "torch.Size([8, 1, 28, 28])\n",
      "6\n",
      "torch.Size([8, 1, 28, 28])\n",
      "7\n",
      "torch.Size([8, 1, 28, 28])\n",
      "8\n",
      "torch.Size([8, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 8 is out of bounds for dimension 0 with size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmlp_sequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\igor-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[14], line 41\u001b[0m, in \u001b[0;36mmlp_sequential\u001b[1;34m(model, dataloader, dev)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\igor-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 30\u001b[0m, in \u001b[0;36mDynamicMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m) \u001b[38;5;66;03m# Flatten the image        \u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Output layer without ReLU activation\u001b[39;00m\n\u001b[0;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](x)\n",
      "File \u001b[1;32mc:\\Users\\igor-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[14], line 31\u001b[0m, in \u001b[0;36mmlp_sequential.<locals>.Catcher.forward\u001b[1;34m(self, inp, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# print(\"inps cache: \", inps[cache['i']])\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[43minps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m inp\n\u001b[0;32m     32\u001b[0m     cache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# cache['attention_mask'] = kwargs['attention_mask']\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# cache['alibi'] = kwargs['alibi']\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 8 is out of bounds for dimension 0 with size 8"
     ]
    }
   ],
   "source": [
    "mlp_sequential(model, testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def mlp_sequential(model, dataloader, dev):\n",
    "    print('Starting ...')\n",
    "\n",
    "    # use_cache = model.config.use_cache\n",
    "    # model.config.use_cache = False\n",
    "    \n",
    "    layers = list(model.modules())[0]\n",
    "    \n",
    "    print(layers)\n",
    "    layers = layers.to(dev)\n",
    "    layers_dict = find_layers(layers); print(layers_dict)\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "    inps = torch.zeros(\n",
    "        (args.batch_size, 28*28), dtype=dtype, device=dev\n",
    "    )\n",
    "\n",
    "    for batch in dataloader:\n",
    "        try:\n",
    "            model(batch[0].to(dev))\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    outs = torch.zeros_like(inps)\n",
    "    # attention_mask = cache['attention_mask']\n",
    "\n",
    "    print('Ready.')\n",
    "    gpts = {}\n",
    "    for i, (layer_name, layer_obj) in enumerate(layers_dict.items()):\n",
    "        if i == len(layers_dict) - 1:\n",
    "            break\n",
    "        layer = layer_obj.to(dev)\n",
    "\n",
    "        # subset = find_layers(layer)\n",
    "        \n",
    "        # gpts = {}\n",
    "        # for name in subset:\n",
    "        gpts[layer_name] = SparseGPT(layer_obj)\n",
    "        print(\"layer_obj \", layer_obj)\n",
    "        def add_batch(layer_name):\n",
    "            def tmp(_, inp, out):\n",
    "                gpts[layer_name].add_batch(inp[0].data, out.data)\n",
    "            return tmp\n",
    "        handles = []\n",
    "        \n",
    "        handles.append(layer_obj.register_forward_hook(add_batch(layer_name)))\n",
    "        for j in range(args.batch_size):\n",
    "            # outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
    "            outs[j] = layer(inps[j].unsqueeze(0))[0]\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "        \n",
    "        print(layer_name)\n",
    "        print('Pruning ...')\n",
    "        sparsity = args.sparsity\n",
    "        gpts[layer_name].fasterprune(\n",
    "                sparsity, prunen=args.prunen, prunem=args.prunem, percdamp=args.percdamp, blocksize=args.blocksize\n",
    "            )\n",
    "        gpts[layer_name].free()\n",
    "\n",
    "        for j in range(args.batch_size):\n",
    "            # outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
    "            outs[j] = layer(inps[j].unsqueeze(0))[0]\n",
    "\n",
    "        layer = layer.cpu()\n",
    "        del layer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        inps, outs = outs, inps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ...\n",
      "DynamicMLP(\n",
      "  (layers): ModuleList(\n",
      "    (0-4): 5 x Linear(in_features=784, out_features=784, bias=True)\n",
      "    (5): Linear(in_features=784, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "{'layers.0': Linear(in_features=784, out_features=784, bias=True), 'layers.1': Linear(in_features=784, out_features=784, bias=True), 'layers.2': Linear(in_features=784, out_features=784, bias=True), 'layers.3': Linear(in_features=784, out_features=784, bias=True), 'layers.4': Linear(in_features=784, out_features=784, bias=True), 'layers.5': Linear(in_features=784, out_features=10, bias=True)}\n",
      "Ready.\n",
      "Linear(in_features=784, out_features=784, bias=True)\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "layer_obj  Linear(in_features=784, out_features=784, bias=True)\n",
      "layers.0\n",
      "Pruning ...\n",
      "time 1.52\n",
      "error 0.0\n",
      "Linear(in_features=784, out_features=784, bias=True)\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "layer_obj  Linear(in_features=784, out_features=784, bias=True)\n",
      "layers.1\n",
      "Pruning ...\n",
      "time 0.40\n",
      "error 6.331102486001328e-05\n",
      "Linear(in_features=784, out_features=784, bias=True)\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "layer_obj  Linear(in_features=784, out_features=784, bias=True)\n",
      "layers.2\n",
      "Pruning ...\n",
      "time 0.40\n",
      "error 0.0017441394738852978\n",
      "Linear(in_features=784, out_features=784, bias=True)\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "layer_obj  Linear(in_features=784, out_features=784, bias=True)\n",
      "layers.3\n",
      "Pruning ...\n",
      "time 0.40\n",
      "error 0.04572782665491104\n",
      "Linear(in_features=784, out_features=784, bias=True)\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "layer_obj  Linear(in_features=784, out_features=784, bias=True)\n",
      "layers.4\n",
      "Pruning ...\n",
      "time 0.40\n",
      "error 1.1236212253570557\n"
     ]
    }
   ],
   "source": [
    "mlp_sequential(model, testloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
