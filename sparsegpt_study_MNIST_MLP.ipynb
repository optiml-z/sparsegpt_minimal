{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\igor-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_runtime.py:185: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from modelutils import *\n",
    "from quant import *\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"block_dynamic_mnist_mlp.pth\"\n",
    "LOAD = True\n",
    "\n",
    "if not(LOAD):\n",
    "    SAVE = True\n",
    "else:\n",
    "    SAVE = False\n",
    "\n",
    "DEBUG = True \n",
    "\n",
    "@dataclass\n",
    "class Args(object):\n",
    "    nsamples: int = 16\n",
    "    sparsity = 0.3\n",
    "    prunen: int = 0\n",
    "    prunem: int = 0\n",
    "    percdamp = .01\n",
    "    blocksize: int = 16\n",
    "    batch_size: int = 64\n",
    "    num_layers: int = 5\n",
    "    input_size: int = 784\n",
    "    output_size: int = 10\n",
    "    minlayer: int = -1\n",
    "    maxlayer: int = 100\n",
    "    prune_only: str = \"\"\n",
    "    invert: bool = True\n",
    "args = Args()\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preparation\n",
    "# Define transformations and load datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "# Confirm if data is loaded\n",
    "len(trainset), len(testset)\n",
    "\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCBlock(nn.Module):\n",
    "    def __init__(self, input_size = 28*28, output_size = 28*28):\n",
    "        super(FCBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, output_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(output_size, output_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=28*28, output_size=10, num_blocks = 4):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.layers = [FCBlock() for i in range(num_blocks)]\n",
    "        out = nn.Linear(input_size, output_size)\n",
    "        self.layers.append(out)\n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "        return self.layers[-1](x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DynamicMLP(num_layers=args.num_layers).to(device)\n",
    "# model = BlockDynamicMLP(input_size=28*28, output_size=10, num_blocks=4, num_layers=args.num_layers).to(device)\n",
    "model = MLP(num_blocks = args.num_layers).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 2\n",
    "train_losses, test_losses = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-4): 5 x FCBlock(\n",
       "    (fc1): Linear(in_features=784, out_features=784, bias=True)\n",
       "    (relu1): ReLU()\n",
       "    (fc2): Linear(in_features=784, out_features=784, bias=True)\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (5): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2.. Train loss: 1.354.. Test loss: 0.836.. Test accuracy: 0.697\n",
      "Epoch 2/2.. Train loss: 0.783.. Test loss: 0.740.. Test accuracy: 0.771\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if LOAD:\n",
    "    PATH = f'./{MODEL_NAME}'\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "else:    \n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images.to(device))\n",
    "            loss = criterion(output, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in testloader:\n",
    "                    log_ps = model(images.to(device))\n",
    "                    test_loss += criterion(log_ps, labels.to(device))\n",
    "                    ps = torch.exp(log_ps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.to(device).view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                    \n",
    "            train_losses.append(running_loss/len(trainloader))\n",
    "            test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "            print(f\"Epoch {e+1}/{epochs}.. \"\n",
    "                f\"Train loss: {running_loss/len(trainloader):.3f}.. \"\n",
    "                f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "                f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
    "\n",
    "    print('Finished Training')\n",
    "    if SAVE:\n",
    "        PATH = f'./{MODEL_NAME}'\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "        plt.plot(train_losses, label='Training loss')\n",
    "        plt.plot(list(map(torch.Tensor.cpu, test_losses)), label='Validation loss')\n",
    "        plt.legend(frameon=False)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples tested = 1000\n",
      "\n",
      "Model Accuracy = 0.775\n"
     ]
    }
   ],
   "source": [
    "def infer_and_compute_accuracy_random_samples(model, dataset, num_samples=1000):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Create a list of indices and shuffle them\n",
    "    indices = list(range(len(dataset)))\n",
    "    np.random.shuffle(indices)\n",
    "    idx = indices[:num_samples]\n",
    "\n",
    "    # Create a DataLoader with SubsetRandomSampler\n",
    "    sampler = SubsetRandomSampler(idx)\n",
    "    testloader_random_samples = DataLoader(dataset, batch_size=64, sampler=sampler)\n",
    "\n",
    "    correct_count, all_count = 0, 0\n",
    "    for images, labels in testloader_random_samples:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images.view(images.shape[0], -1).to(device))\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_count += (predicted == labels.to(device)).sum().item()\n",
    "        all_count += labels.size(0)\n",
    "    \n",
    "    print(\"Number of Samples tested =\", all_count)\n",
    "    print(\"\\nModel Accuracy =\", (correct_count / all_count))\n",
    "\n",
    "# Assuming testset is your test dataset\n",
    "# Call the function\n",
    "infer_and_compute_accuracy_random_samples(model, testset, num_samples=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](torch.randn(1, 28*28).to(device)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layers.0.fc1': Linear(in_features=784, out_features=784, bias=True),\n",
       " 'layers.0.fc2': Linear(in_features=784, out_features=784, bias=True),\n",
       " 'layers.1.fc1': Linear(in_features=784, out_features=784, bias=True),\n",
       " 'layers.1.fc2': Linear(in_features=784, out_features=784, bias=True),\n",
       " 'layers.2.fc1': Linear(in_features=784, out_features=784, bias=True),\n",
       " 'layers.2.fc2': Linear(in_features=784, out_features=784, bias=True),\n",
       " 'layers.3.fc1': Linear(in_features=784, out_features=784, bias=True),\n",
       " 'layers.3.fc2': Linear(in_features=784, out_features=784, bias=True),\n",
       " 'layers.4.fc1': Linear(in_features=784, out_features=784, bias=True),\n",
       " 'layers.4.fc2': Linear(in_features=784, out_features=784, bias=True),\n",
       " 'layers.5': Linear(in_features=784, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseGPT:\n",
    "\n",
    "    def __init__(self, layer):\n",
    "        self.layer = layer\n",
    "        print(layer)\n",
    "        print(type(layer))\n",
    "        self.dev = self.layer.weight.device\n",
    "        W = layer.weight.data.clone()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            W = W.flatten(1)\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        self.rows = W.shape[0]\n",
    "        self.columns = W.shape[1]\n",
    "        self.H = torch.zeros((self.columns, self.columns), device=self.dev)\n",
    "        self.nsamples = 0\n",
    "\n",
    "    def add_batch(self, inp, out, blocksize=args.blocksize):\n",
    "        if DEBUG:\n",
    "            self.inp1 = inp\n",
    "            self.out1 = out\n",
    "        if len(inp.shape) == 2:\n",
    "            inp = inp.unsqueeze(0)\n",
    "        tmp = inp.shape[0]\n",
    "        if isinstance(self.layer, nn.Linear) or isinstance(self.layer, transformers.Conv1D):\n",
    "            if len(inp.shape) == 3:\n",
    "                inp = inp.reshape((-1, inp.shape[-1]))\n",
    "            inp = inp.t()\n",
    "        self.H *= self.nsamples / (self.nsamples + tmp)\n",
    "        self.nsamples += tmp\n",
    "        inp = math.sqrt(2 / self.nsamples) * inp.float()\n",
    "        self.H += inp.matmul(inp.t())\n",
    "\n",
    "    def fasterprune(\n",
    "        self, sparsity, prunen=0, prunem=0, blocksize=args.blocksize, percdamp=.01\n",
    "    ):\n",
    "        W = self.layer.weight.data.clone()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            W = W.flatten(1)\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        W = W.float()\n",
    "\n",
    "        if hasattr(self, 'quantizer'):\n",
    "            if not self.quantizer.ready():\n",
    "                self.quantizer.find_params(W, weight=True)\n",
    "\n",
    "        tick = time.time()\n",
    "\n",
    "        H = self.H\n",
    "        del self.H\n",
    "        dead = torch.diag(H) == 0\n",
    "        H[dead, dead] = 1\n",
    "        W[:, dead] = 0\n",
    "\n",
    "        Losses = torch.zeros(self.rows, device=self.dev)\n",
    "\n",
    "        damp = percdamp * torch.mean(torch.diag(H))\n",
    "        diag = torch.arange(self.columns, device=self.dev)\n",
    "        H[diag, diag] += damp\n",
    "        H = torch.linalg.cholesky(H)\n",
    "        H = torch.cholesky_inverse(H)\n",
    "        H = torch.linalg.cholesky(H, upper=True)\n",
    "        Hinv = H\n",
    "\n",
    "        mask = None\n",
    "\n",
    "        for i1 in range(0, self.columns, blocksize):\n",
    "            i2 = min(i1 + blocksize, self.columns)\n",
    "            count = i2 - i1\n",
    "\n",
    "            W1 = W[:, i1:i2].clone()\n",
    "            Q1 = torch.zeros_like(W1)\n",
    "            Err1 = torch.zeros_like(W1)\n",
    "            Losses1 = torch.zeros_like(W1)\n",
    "            Hinv1 = Hinv[i1:i2, i1:i2]\n",
    "\n",
    "            if prunen == 0: \n",
    "                if mask is not None:\n",
    "                    mask1 = mask[:, i1:i2]\n",
    "                else:\n",
    "                    tmp = W1 ** 2 / (torch.diag(Hinv1).reshape((1, -1))) ** 2\n",
    "                    thresh = torch.sort(tmp.flatten())[0][int(tmp.numel() * sparsity)]\n",
    "                    mask1 = tmp <= thresh\n",
    "            else:\n",
    "                mask1 = torch.zeros_like(W1) == 1\n",
    "\n",
    "            for i in range(count):\n",
    "                w = W1[:, i]\n",
    "                d = Hinv1[i, i]\n",
    "\n",
    "                if prunen != 0 and i % prunem == 0:\n",
    "                    tmp = W1[:, i:(i + prunem)] ** 2 / (torch.diag(Hinv1)[i:(i + prunem)].reshape((1, -1))) ** 2\n",
    "                    mask1.scatter_(1, i + torch.topk(tmp, prunen, dim=1, largest=False)[1], True)\n",
    "\n",
    "                q = w.clone()\n",
    "                q[mask1[:, i]] = 0\n",
    "\n",
    "                if hasattr(self, 'quantizer'):\n",
    "                    q = quantize(\n",
    "                        q.unsqueeze(1), self.quantizer.scale, self.quantizer.zero, self.quantizer.maxq\n",
    "                    ).flatten()\n",
    "\n",
    "                Q1[:, i] = q\n",
    "                Losses1[:, i] = (w - q) ** 2 / d ** 2\n",
    "\n",
    "                err1 = (w - q) / d\n",
    "                W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))\n",
    "                Err1[:, i] = err1\n",
    "\n",
    "            W[:, i1:i2] = Q1\n",
    "            Losses += torch.sum(Losses1, 1) / 2\n",
    "\n",
    "            W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])\n",
    "\n",
    "            if DEBUG:\n",
    "                self.layer.weight.data[:, :i2] = W[:, :i2]\n",
    "                self.layer.weight.data[:, i2:] = W[:, i2:]\n",
    "                print(torch.sum((self.layer(self.inp1) - self.out1) ** 2))\n",
    "                print(torch.sum(Losses))\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        print('time %.2f' % (time.time() - tick))\n",
    "        print('error', torch.sum(Losses).item())\n",
    "\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        self.layer.weight.data = W.reshape(self.layer.weight.shape).to(self.layer.weight.data.dtype)\n",
    "        if DEBUG:\n",
    "            print(torch.sum((self.layer(self.inp1) - self.out1) ** 2))\n",
    "\n",
    "    def free(self):\n",
    "        if DEBUG:\n",
    "            self.inp1 = None\n",
    "            self.out1 = None\n",
    "        self.H = None\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(testset)))\n",
    "np.random.shuffle(indices)\n",
    "idx = indices[:1000]\n",
    "sampler = SubsetRandomSampler(idx)\n",
    "calibration_loader = DataLoader(testset, batch_size=args.nsamples, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def mlp_sequential(model, dataloader, dev):\n",
    "    print('Starting ...')\n",
    "\n",
    "    # use_cache = model.config.use_cache\n",
    "    # model.config.use_cache = False\n",
    "    # layers = model.transformer.h\n",
    "    layers = model.layers\n",
    "    print(\"layers: \", layers)\n",
    "    # model.transformer.word_embeddings = model.transformer.word_embeddings.to(dev)\n",
    "    # model.transformer.word_embeddings_layernorm = model.transformer.word_embeddings_layernorm.to(dev)\n",
    "    layers[0] = layers[0].to(dev)\n",
    "    \n",
    "    print(\"layers[0]: \", layers[0])\n",
    "    layers[0] = layers[0].to(dev)\n",
    "\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "    inps = torch.zeros(\n",
    "        (args.nsamples, args.nsamples, 28*28), dtype=dtype, device=dev\n",
    "    )\n",
    "    # cache = {'i': 0, 'attention_mask': None, 'alibi': None}\n",
    "    cache = {'i': 0}\n",
    "\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "        def forward(self, inp, **kwargs):\n",
    "            # print(\"inps cache: \", inps[cache['i']])\n",
    "            # if cache['i'] == args.nsamples - 1:\n",
    "            #     raise ValueError\n",
    "            if cache['i'] < args.nsamples:\n",
    "                inps[cache['i']] = inp\n",
    "                cache['i'] += 1\n",
    "            # cache['attention_mask'] = kwargs['attention_mask']\n",
    "            # cache['alibi'] = kwargs['alibi']\n",
    "            raise ValueError\n",
    "    layers[0] = Catcher(layers[0])\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        try:\n",
    "            print(i)\n",
    "            # print(batch[0].shape)\n",
    "            model(batch[0].to(dev))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    layers[0] = layers[0].module\n",
    "\n",
    "    layers[0] = layers[0].cpu()\n",
    "    # model.transformer.word_embeddings = model.transformer.word_embeddings.cpu()\n",
    "    # model.transformer.word_embeddings_layernorm = model.transformer.word_embeddings_layernorm.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    outs = torch.zeros_like(inps)\n",
    "    # attention_mask = cache['attention_mask']\n",
    "    # alibi = cache['alibi']\n",
    "\n",
    "    print('Ready.')\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i].to(dev)\n",
    "        print(i, \" \", layer)\n",
    "        subset = find_layers(layer)\n",
    "        gpts = {}\n",
    "        for name in subset:\n",
    "            if (not (args.minlayer <= i < args.maxlayer and args.prune_only in name)) == (not args.invert):\n",
    "                continue\n",
    "            gpts[name] = SparseGPT(subset[name])\n",
    "\n",
    "        def add_batch(name):\n",
    "            def tmp(_, inp, out):\n",
    "                gpts[name].add_batch(inp[0].data, out.data)\n",
    "            return tmp\n",
    "        handles = []\n",
    "        for name in gpts:\n",
    "            handles.append(subset[name].register_forward_hook(add_batch(name)))\n",
    "        for j in range(args.nsamples):\n",
    "            # outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, alibi=alibi)[0]\n",
    "            outs[j] = layer(inps[j].unsqueeze(0))[0]\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "        for name in gpts:\n",
    "            print(i, name)\n",
    "            print('pruning ...')\n",
    "            gpts[name].fasterprune(\n",
    "                args.sparsity, prunen=args.prunen, prunem=args.prunem, percdamp=args.percdamp\n",
    "            )\n",
    "        for j in range(args.nsamples):\n",
    "            # outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, alibi=alibi)[0]\n",
    "            outs[j] = layer(inps[j].unsqueeze(0))[0]\n",
    "\n",
    "        layers[i] = layer.cpu()\n",
    "        del gpts \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        inps, outs = outs, inps\n",
    "\n",
    "    # model.config.use_cache = use_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCBlock(\n",
       "  (fc1): Linear(in_features=784, out_features=784, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=784, out_features=784, bias=True)\n",
       "  (relu2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ...\n",
      "layers:  ModuleList(\n",
      "  (0-4): 5 x FCBlock(\n",
      "    (fc1): Linear(in_features=784, out_features=784, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (fc2): Linear(in_features=784, out_features=784, bias=True)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (5): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "layers[0]:  FCBlock(\n",
      "  (fc1): Linear(in_features=784, out_features=784, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=784, out_features=784, bias=True)\n",
      "  (relu2): ReLU()\n",
      ")\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "Ready.\n",
      "0   FCBlock(\n",
      "  (fc1): Linear(in_features=784, out_features=784, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=784, out_features=784, bias=True)\n",
      "  (relu2): ReLU()\n",
      ")\n",
      "1   FCBlock(\n",
      "  (fc1): Linear(in_features=784, out_features=784, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=784, out_features=784, bias=True)\n",
      "  (relu2): ReLU()\n",
      ")\n",
      "2   FCBlock(\n",
      "  (fc1): Linear(in_features=784, out_features=784, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=784, out_features=784, bias=True)\n",
      "  (relu2): ReLU()\n",
      ")\n",
      "3   FCBlock(\n",
      "  (fc1): Linear(in_features=784, out_features=784, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=784, out_features=784, bias=True)\n",
      "  (relu2): ReLU()\n",
      ")\n",
      "4   FCBlock(\n",
      "  (fc1): Linear(in_features=784, out_features=784, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=784, out_features=784, bias=True)\n",
      "  (relu2): ReLU()\n",
      ")\n",
      "5   Linear(in_features=784, out_features=10, bias=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (784) must match the existing size (10) at non-singleton dimension 1.  Target sizes: [16, 784].  Tensor sizes: [16, 10]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmlp_sequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalibration_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\igor-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[21], line 78\u001b[0m, in \u001b[0;36mmlp_sequential\u001b[1;34m(model, dataloader, dev)\u001b[0m\n\u001b[0;32m     75\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(subset[name]\u001b[38;5;241m.\u001b[39mregister_forward_hook(add_batch(name)))\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mnsamples):\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, alibi=alibi)[0]\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     \u001b[43mouts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m layer(inps[j]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m handles:\n\u001b[0;32m     80\u001b[0m     h\u001b[38;5;241m.\u001b[39mremove()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (784) must match the existing size (10) at non-singleton dimension 1.  Target sizes: [16, 784].  Tensor sizes: [16, 10]"
     ]
    }
   ],
   "source": [
    "mlp_sequential(model, calibration_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
