{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\igor-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_runtime.py:185: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from modelutils import *\n",
    "from quant import *\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"block_dynamic_mnist_mlp.pth\"\n",
    "LOAD = True\n",
    "\n",
    "if not(LOAD):\n",
    "    SAVE = True\n",
    "else:\n",
    "    SAVE = False\n",
    "\n",
    "DEBUG = True \n",
    "\n",
    "@dataclass\n",
    "class Args(object):\n",
    "    nsamples: int = 16\n",
    "    sparsity = 0.3\n",
    "    prunen: int = 0\n",
    "    prunem: int = 0\n",
    "    percdamp = .01\n",
    "    blocksize: int = 16\n",
    "    batch_size: int = 16\n",
    "    num_layers: int = 4\n",
    "    input_size: int = 784\n",
    "    output_size: int = 10\n",
    "    minlayer: int = -1\n",
    "    maxlayer: int = 1000\n",
    "    prune_only: str = \"\"\n",
    "    invert: bool = True\n",
    "args = Args()\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preparation\n",
    "# Define transformations and load datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "# Confirm if data is loaded\n",
    "len(trainset), len(testset)\n",
    "\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCBlock(nn.Module):\n",
    "    def __init__(self, input_size = 28*28, output_size = 28*28):\n",
    "        super(FCBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, output_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(output_size, output_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=28*28, output_size=10, num_blocks = 4):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.layers = [FCBlock() for i in range(num_blocks)]\n",
    "        self.out = nn.Linear(input_size, output_size)\n",
    "        self.layers.append(self.out)\n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "        return self.layers[-1](x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DynamicMLP(num_layers=args.num_layers).to(device)\n",
    "# model = BlockDynamicMLP(input_size=28*28, output_size=10, num_blocks=4, num_layers=args.num_layers).to(device)\n",
    "model = MLP().to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 2\n",
    "train_losses, test_losses = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-3): 4 x FCBlock(\n",
       "    (fc1): Linear(in_features=784, out_features=784, bias=True)\n",
       "    (relu1): ReLU()\n",
       "    (fc2): Linear(in_features=784, out_features=784, bias=True)\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (4): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD:\n",
    "    PATH = f'./{MODEL_NAME}'\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "else:    \n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images.to(device))\n",
    "            loss = criterion(output, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in testloader:\n",
    "                    log_ps = model(images.to(device))\n",
    "                    test_loss += criterion(log_ps, labels.to(device))\n",
    "                    ps = torch.exp(log_ps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.to(device).view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                    \n",
    "            train_losses.append(running_loss/len(trainloader))\n",
    "            test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "            print(f\"Epoch {e+1}/{epochs}.. \"\n",
    "                f\"Train loss: {running_loss/len(trainloader):.3f}.. \"\n",
    "                f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "                f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
    "\n",
    "    print('Finished Training')\n",
    "    if SAVE:\n",
    "        PATH = f'./{MODEL_NAME}'\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "        plt.plot(train_losses, label='Training loss')\n",
    "        plt.plot(list(map(torch.Tensor.cpu, test_losses)), label='Validation loss')\n",
    "        plt.legend(frameon=False)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples tested = 1000\n",
      "\n",
      "Model Accuracy = 0.7\n"
     ]
    }
   ],
   "source": [
    "def infer_and_compute_accuracy_random_samples(model, dataset, num_samples=1000):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Create a list of indices and shuffle them\n",
    "    indices = list(range(len(dataset)))\n",
    "    np.random.shuffle(indices)\n",
    "    idx = indices[:num_samples]\n",
    "\n",
    "    # Create a DataLoader with SubsetRandomSampler\n",
    "    sampler = SubsetRandomSampler(idx)\n",
    "    testloader_random_samples = DataLoader(dataset, batch_size=64, sampler=sampler)\n",
    "\n",
    "    correct_count, all_count = 0, 0\n",
    "    for images, labels in testloader_random_samples:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images.view(images.shape[0], -1).to(device))\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_count += (predicted == labels.to(device)).sum().item()\n",
    "        all_count += labels.size(0)\n",
    "    \n",
    "    print(\"Number of Samples tested =\", all_count)\n",
    "    print(\"\\nModel Accuracy =\", (correct_count / all_count))\n",
    "\n",
    "# Assuming testset is your test dataset\n",
    "# Call the function\n",
    "infer_and_compute_accuracy_random_samples(model, testset, num_samples=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](torch.randn(1, 28*28).to(device)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-2): 3 x FCBlock(\n",
       "    (fc1): Linear(in_features=784, out_features=784, bias=True)\n",
       "    (relu1): ReLU()\n",
       "    (fc2): Linear(in_features=784, out_features=784, bias=True)\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (3): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseGPT:\n",
    "\n",
    "    def __init__(self, layer):\n",
    "        self.layer = layer\n",
    "        print(layer)\n",
    "        print(type(layer))\n",
    "        self.dev = self.layer.weight.device\n",
    "        W = layer.weight.data.clone()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            W = W.flatten(1)\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        self.rows = W.shape[0]\n",
    "        self.columns = W.shape[1]\n",
    "        self.H = torch.zeros((self.columns, self.columns), device=self.dev)\n",
    "        self.nsamples = 0\n",
    "\n",
    "    def add_batch(self, inp, out, blocksize=1024):\n",
    "        if DEBUG:\n",
    "            self.inp1 = inp\n",
    "            self.out1 = out\n",
    "        if len(inp.shape) == 2:\n",
    "            inp = inp.unsqueeze(0)\n",
    "        tmp = inp.shape[0]\n",
    "        if isinstance(self.layer, nn.Linear) or isinstance(self.layer, transformers.Conv1D):\n",
    "            if len(inp.shape) == 3:\n",
    "                inp = inp.reshape((-1, inp.shape[-1]))\n",
    "            inp = inp.t()\n",
    "        self.H *= self.nsamples / (self.nsamples + tmp)\n",
    "        self.nsamples += tmp\n",
    "        inp = math.sqrt(2 / self.nsamples) * inp.float()\n",
    "        self.H += inp.matmul(inp.t())\n",
    "\n",
    "    def fasterprune(\n",
    "        self, sparsity, prunen=0, prunem=0, blocksize=128, percdamp=.01\n",
    "    ):\n",
    "        W = self.layer.weight.data.clone()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            W = W.flatten(1)\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        W = W.float()\n",
    "\n",
    "        if hasattr(self, 'quantizer'):\n",
    "            if not self.quantizer.ready():\n",
    "                self.quantizer.find_params(W, weight=True)\n",
    "\n",
    "        tick = time.time()\n",
    "\n",
    "        H = self.H\n",
    "        del self.H\n",
    "        dead = torch.diag(H) == 0\n",
    "        H[dead, dead] = 1\n",
    "        W[:, dead] = 0\n",
    "\n",
    "        Losses = torch.zeros(self.rows, device=self.dev)\n",
    "\n",
    "        damp = percdamp * torch.mean(torch.diag(H))\n",
    "        diag = torch.arange(self.columns, device=self.dev)\n",
    "        H[diag, diag] += damp\n",
    "        H = torch.linalg.cholesky(H)\n",
    "        H = torch.cholesky_inverse(H)\n",
    "        H = torch.linalg.cholesky(H, upper=True)\n",
    "        Hinv = H\n",
    "\n",
    "        mask = None\n",
    "\n",
    "        for i1 in range(0, self.columns, blocksize):\n",
    "            i2 = min(i1 + blocksize, self.columns)\n",
    "            count = i2 - i1\n",
    "\n",
    "            W1 = W[:, i1:i2].clone()\n",
    "            Q1 = torch.zeros_like(W1)\n",
    "            Err1 = torch.zeros_like(W1)\n",
    "            Losses1 = torch.zeros_like(W1)\n",
    "            Hinv1 = Hinv[i1:i2, i1:i2]\n",
    "\n",
    "            if prunen == 0: \n",
    "                if mask is not None:\n",
    "                    mask1 = mask[:, i1:i2]\n",
    "                else:\n",
    "                    tmp = W1 ** 2 / (torch.diag(Hinv1).reshape((1, -1))) ** 2\n",
    "                    thresh = torch.sort(tmp.flatten())[0][int(tmp.numel() * sparsity)]\n",
    "                    mask1 = tmp <= thresh\n",
    "            else:\n",
    "                mask1 = torch.zeros_like(W1) == 1\n",
    "\n",
    "            for i in range(count):\n",
    "                w = W1[:, i]\n",
    "                d = Hinv1[i, i]\n",
    "\n",
    "                if prunen != 0 and i % prunem == 0:\n",
    "                    tmp = W1[:, i:(i + prunem)] ** 2 / (torch.diag(Hinv1)[i:(i + prunem)].reshape((1, -1))) ** 2\n",
    "                    mask1.scatter_(1, i + torch.topk(tmp, prunen, dim=1, largest=False)[1], True)\n",
    "\n",
    "                q = w.clone()\n",
    "                q[mask1[:, i]] = 0\n",
    "\n",
    "                if hasattr(self, 'quantizer'):\n",
    "                    q = quantize(\n",
    "                        q.unsqueeze(1), self.quantizer.scale, self.quantizer.zero, self.quantizer.maxq\n",
    "                    ).flatten()\n",
    "\n",
    "                Q1[:, i] = q\n",
    "                Losses1[:, i] = (w - q) ** 2 / d ** 2\n",
    "\n",
    "                err1 = (w - q) / d\n",
    "                W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))\n",
    "                Err1[:, i] = err1\n",
    "\n",
    "            W[:, i1:i2] = Q1\n",
    "            Losses += torch.sum(Losses1, 1) / 2\n",
    "\n",
    "            W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])\n",
    "\n",
    "            if DEBUG:\n",
    "                self.layer.weight.data[:, :i2] = W[:, :i2]\n",
    "                self.layer.weight.data[:, i2:] = W[:, i2:]\n",
    "                print(torch.sum((self.layer(self.inp1) - self.out1) ** 2))\n",
    "                print(torch.sum(Losses))\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        print('time %.2f' % (time.time() - tick))\n",
    "        print('error', torch.sum(Losses).item())\n",
    "\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        self.layer.weight.data = W.reshape(self.layer.weight.shape).to(self.layer.weight.data.dtype)\n",
    "        if DEBUG:\n",
    "            print(torch.sum((self.layer(self.inp1) - self.out1) ** 2))\n",
    "\n",
    "    def free(self):\n",
    "        if DEBUG:\n",
    "            self.inp1 = None\n",
    "            self.out1 = None\n",
    "        self.H = None\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def mlp_sequential(model, dataloader, dev):\n",
    "    print('Starting ...')\n",
    "\n",
    "    # use_cache = model.config.use_cache\n",
    "    # model.config.use_cache = False\n",
    "    # layers = model.transformer.h\n",
    "    layers = model.layers\n",
    "    print(\"layers: \", layers)\n",
    "    # model.transformer.word_embeddings = model.transformer.word_embeddings.to(dev)\n",
    "    # model.transformer.word_embeddings_layernorm = model.transformer.word_embeddings_layernorm.to(dev)\n",
    "    layers[0] = layers[0].to(dev)\n",
    "    \n",
    "    print(\"layers[0]: \", layers[0])\n",
    "    layers[0] = layers[0].to(dev)\n",
    "\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "    inps = torch.zeros(\n",
    "        (args.nsamples, args.nsamples, 28*28), dtype=dtype, device=dev\n",
    "    )\n",
    "    # cache = {'i': 0, 'attention_mask': None, 'alibi': None}\n",
    "    cache = {'i': 0}\n",
    "\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "        def forward(self, inp, **kwargs):\n",
    "            # print(\"inps cache: \", inps[cache['i']])\n",
    "            # if cache['i'] == args.nsamples - 1:\n",
    "            #     raise ValueError\n",
    "            if cache['i'] < args.nsamples:\n",
    "                inps[cache['i']] = inp\n",
    "                cache['i'] += 1\n",
    "            # cache['attention_mask'] = kwargs['attention_mask']\n",
    "            # cache['alibi'] = kwargs['alibi']\n",
    "            raise ValueError\n",
    "    layers[0] = Catcher(layers[0])\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        try:\n",
    "            # print(i)\n",
    "            # print(batch[0].shape)\n",
    "            model(batch[0].to(dev))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    layers[0] = layers[0].module\n",
    "\n",
    "    layers[0] = layers[0].cpu()\n",
    "    # model.transformer.word_embeddings = model.transformer.word_embeddings.cpu()\n",
    "    # model.transformer.word_embeddings_layernorm = model.transformer.word_embeddings_layernorm.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    outs = torch.zeros_like(inps)\n",
    "    # attention_mask = cache['attention_mask']\n",
    "    # alibi = cache['alibi']\n",
    "\n",
    "    print('Ready.')\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i].to(dev)\n",
    "\n",
    "        subset = find_layers(layer)\n",
    "        gpts = {}\n",
    "        for name in subset:\n",
    "            if (not (args.minlayer <= i < args.maxlayer and args.prune_only in name)) == (not args.invert):\n",
    "                continue\n",
    "            gpts[name] = SparseGPT(subset[name])\n",
    "\n",
    "        def add_batch(name):\n",
    "            def tmp(_, inp, out):\n",
    "                gpts[name].add_batch(inp[0].data, out.data)\n",
    "            return tmp\n",
    "        handles = []\n",
    "        for name in gpts:\n",
    "            handles.append(subset[name].register_forward_hook(add_batch(name)))\n",
    "        for j in range(args.nsamples):\n",
    "            # outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, alibi=alibi)[0]\n",
    "            outs[j] = layer(inps[j].unsqueeze(0))[0]\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "        for name in gpts:\n",
    "            print(i, name)\n",
    "            print('pruning ...')\n",
    "            gpts[name].fasterprune(\n",
    "                args.sparsity, prunen=args.prunen, prunem=args.prunem, percdamp=args.percdamp\n",
    "            )\n",
    "        for j in range(args.nsamples):\n",
    "            # outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, alibi=alibi)[0]\n",
    "            outs[j] = layer(inps[j].unsqueeze(0))[0]\n",
    "\n",
    "        layers[i] = layer.cpu()\n",
    "        del gpts \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        inps, outs = outs, inps\n",
    "\n",
    "    # model.config.use_cache = use_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCBlock(\n",
       "  (fc1): Linear(in_features=784, out_features=784, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=784, out_features=784, bias=True)\n",
       "  (relu2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ...\n",
      "layers:  ModuleList(\n",
      "  (0): Catcher(\n",
      "    (module): Catcher(\n",
      "      (module): Catcher(\n",
      "        (module): FCBlock(\n",
      "          (fc1): Linear(in_features=784, out_features=784, bias=True)\n",
      "          (relu1): ReLU()\n",
      "          (fc2): Linear(in_features=784, out_features=784, bias=True)\n",
      "          (relu2): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1-3): 3 x FCBlock(\n",
      "    (fc1): Linear(in_features=784, out_features=784, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (fc2): Linear(in_features=784, out_features=784, bias=True)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (4): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "layers[0]:  Catcher(\n",
      "  (module): Catcher(\n",
      "    (module): Catcher(\n",
      "      (module): FCBlock(\n",
      "        (fc1): Linear(in_features=784, out_features=784, bias=True)\n",
      "        (relu1): ReLU()\n",
      "        (fc2): Linear(in_features=784, out_features=784, bias=True)\n",
      "        (relu2): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.cuda.FloatTensor{[16, 784]}, size=[784]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmlp_sequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\igor-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[21], line 43\u001b[0m, in \u001b[0;36mmlp_sequential\u001b[1;34m(model, dataloader, dev)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;66;03m# print(i)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;66;03m# print(batch[0].shape)\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\igor-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[4], line 31\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m---> 31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](x)\n",
      "File \u001b[1;32mc:\\Users\\igor-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[21], line 33\u001b[0m, in \u001b[0;36mmlp_sequential.<locals>.Catcher.forward\u001b[1;34m(self, inp, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# print(\"inps cache: \", inps[cache['i']])\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# if cache['i'] == args.nsamples - 1:\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m#     raise ValueError\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# if cache['i'] < args.nsamples:\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     \u001b[43minps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m inp\n\u001b[0;32m     34\u001b[0m     cache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# cache['attention_mask'] = kwargs['attention_mask']\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# cache['alibi'] = kwargs['alibi']\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expand(torch.cuda.FloatTensor{[16, 784]}, size=[784]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)"
     ]
    }
   ],
   "source": [
    "mlp_sequential(model, testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
